# インデックス更新のオーバーヘッド実験

## 検証内容

大量の INSERT をしている最中に、検索（SELECT）がどれだけ遅くなるか。

## 準備

高負荷の作り方: pgbench を使って、ひたすら INSERT と UPDATE をループさせる。

## ポイント

- インデックスが多いテーブルほど、INSERT が目に見えて遅くなる様子。
- 書き込み負荷が激しい時に、読み取りクエリが「チェックポイント（ディスクへの書き出し）」のタイミングでガクッと遅くなる現象（ストール）。

## コマンド

```bash
# コンテナ起動
$ docker compose up -d
$ docker container ps

# ログの確認
$ docker logs postgres_container_index
$ docker logs -f postgres_container_index

# DBへの接続
$ psql -h localhost -p 5433 -U test-user -d training_db

```

```bash
# 実験
$ psql -h localhost -p 5433 -U test-user -d training_db -c "
SELECT 
    date_trunc('month', created_date_time) as month,
    count(*),
    avg(balance)
FROM sample_tbl 
WHERE id LIKE '%a%b%c%'  -- あえてインデックスが効かない、重い文字列検索
GROUP BY 1;"

# 実験２
$ pgbench -h localhost -p 5433 -U test-user -d training_db -c 5 -t 20 -f scripts/heavy_select.sql
Password: 
pgbench (16.11 (Ubuntu 16.11-0ubuntu0.24.04.1))
pgbench: pghost: localhost pgport: 5433 nclients: 5 nxacts: 20 dbName: training_db
starting vacuum...pgbench: error: ERROR:  relation "pgbench_branches" does not exist
pgbench: detail: (ignoring this error and continuing anyway)
pgbench: error: ERROR:  relation "pgbench_tellers" does not exist
pgbench: detail: (ignoring this error and continuing anyway)
pgbench: error: ERROR:  relation "pgbench_history" does not exist
pgbench: detail: (ignoring this error and continuing anyway)
end.
(略)
transaction type: scripts/heavy_select.sql
scaling factor: 1
query mode: simple
number of clients: 5
number of threads: 1
maximum number of tries: 1
number of transactions per client: 20
number of transactions actually processed: 100/100
number of failed transactions: 0 (0.000%)
latency average = 18910.762 ms
initial connection time = 38.388 ms
tps = 0.264400 (without initial connection time)

# index確認
training_db=#
EXPLAIN ANALYZE SELECT count(*) FROM sample_tbl 
WHERE created_date_time BETWEEN '2026-01-01 00:00:00' AND '2026-01-31 23:59:59';
QUERYPLAN 
 Aggregate  (cost=2761.41..2761.42 rows=1 width=8) (actual time=11.322..11.323 rows=1 loops=1)
   ->  Index Only Scan using idx_sample_created_at on sample_tbl  (cost=0.42..2555.53 rows=82355 width=0) (actual time=0.015..8.062 rows=80827 loops=1)
         Index Cond: ((created_date_time >= '2026-01-01 00:00:00'::timestamp without time zone) AND (created_date_time <= '2026-01-31 23:59:59'::timestamp without time zone))
         Heap Fetches: 0
 Planning Time: 0.147 ms
 Execution Time: 11.353 ms
(6 rows)

# index前後の書き込み速度
## table作成
training_db=# CREATE TABLE test_write_speed (
    id SERIAL PRIMARY KEY
    , val TEXT
    , created_at TIMESTAMP
);
CREATE TABLE

## データの挿入（1秒未満）
training_db=# INSERT INTO test_write_speed (val, created_at)
SELECT md5(random()::text), now() FROM generate_series(1, 100000);
INSERT 0 100000

## index付与
training_db=# CREATE INDEX idx_1 ON test_write_speed (val);
CREATE INDEX
training_db=# CREATE INDEX idx_2 ON test_write_speed (created_at);
CREATE INDEX
training_db=# CREATE INDEX idx_3 ON test_write_speed (val, created_at);
CREATE INDEX

## データの挿入（2秒はかからないぐらい）
training_db=# INSERT INTO test_write_speed (val, created_at)
SELECT md5(random()::text), now() FROM generate_series(1, 100000);
INSERT 0 100000
training_db=# 
```

```bash
# 初期化
# 1. コンテナ停止 ＆ ボリュームの完全削除
docker compose down -v

# 2. ローカルの pgdata フォルダを物理削除（念のため）
sudo rm -rf ./pgdata

# 3. 起動（これで ddl.sql が実行されます）
docker compose up -d
```
